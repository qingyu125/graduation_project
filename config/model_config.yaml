model:
  name: "codellama/CodeLlama-7b-hf"
  quantize:
    type: "4bit"  # 4bit/8bit/none
    load_in_4bit: True
    bnb_4bit_use_double_quant: True
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "float16"

lora:
  rank: 8
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj", "v_proj"]  # CodeLlama注意力层目标模块
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 2e-4
  num_train_epochs: 15
  fp16: True
  logging_steps: 10
  save_steps: 50
  evaluation_strategy: "steps"
  eval_steps: 50
  load_best_model_at_end: True

generation:
  max_new_tokens: 256
  temperature: 0.3
  top_p: 0.9
  do_sample: False